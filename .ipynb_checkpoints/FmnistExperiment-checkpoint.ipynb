{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of training data  60000\n"
     ]
    }
   ],
   "source": [
    "# datasets\n",
    "trainset = torchvision.datasets.FashionMNIST('./data',\n",
    "    download=False,\n",
    "    train=True,\n",
    "    transform=transform)\n",
    "print(' Number of training data ', len(trainset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of testing data  10000\n"
     ]
    }
   ],
   "source": [
    "testset = torchvision.datasets.FashionMNIST('./data',\n",
    "    download=False,\n",
    "    train=False,\n",
    "    transform=transform)\n",
    "print(' Number of testing data ',len(testset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU(inplace=True)\n",
      "  (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (4): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (5): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (6): ReLU(inplace=True)\n",
      "  (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "cnn_layers =  nn.Sequential(\n",
    "            # Defining a 2D convolution layer\n",
    "            # torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "            # in_channels (int) – Number of channels in the input image. For B&W it is 1.\n",
    "            # out_channels (int) – Number of channels produced by the convolution. 4 filters\n",
    "            # kernel_size (int or tuple) – Size of the convolving kernel (3x3)\n",
    "            # stride (int or tuple, optional) – Stride of the convolution\n",
    "            # padding (int or tuple, optional) – Padding of 1 added to both sides of the input\n",
    "            # example x1 = (n, c=1 , h=28 , w=28 )\n",
    "            nn.Conv2d(1, 4, kernel_size=3, stride=1, padding=1), #in_channels = 1 is a data dependent hyperparameter. It is 1 because the images are in grayscale\n",
    "            # x2 = (n, c=4 , h=28 , w=28 )\n",
    "            nn.BatchNorm2d(4), # Normalize output from the activation function. \n",
    "            nn.ReLU(inplace=True), # negative elements to zero\n",
    "            # x2 = (n, c=4 , h=28 , w=28 )\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #Stride is the number of pixels shifts over the input matrix. When the stride is 1 then we move the filters to 1 pixel at a time. When the stride is 2 then we move the filters to 2 pixels at a time and so on\n",
    "            # x3 = (n, c=4 , h=14 , w=14 )\n",
    "            # Defining another 2D convolution layer\n",
    "            nn.Conv2d(4, 4, kernel_size=3, stride=1, padding=1),\n",
    "            # x3 = (n, c=4 , h=14 , w=14 )\n",
    "            nn.BatchNorm2d(4), # 4 features\n",
    "            nn.ReLU(inplace=True), # inplace = True will modify the input directly, without allocating any additional output.\n",
    "            # x3 = (n, c=4 , h=14 , w=14 )\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), # Downsamples the input representation by taking the maximum value\n",
    "            # x4 = (n, c=4 , h=7 , w=7 )\n",
    "        )\n",
    "        # outputSize = floor[(inputSize - filterSize + 2 * padding) / stride] + 1\n",
    "print(cnn_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_layers = nn.Sequential(\n",
    "            nn.Linear(4 * 7 * 7, 10) \n",
    "#10 outputs because MNIST Fashion has 10 different classes. It is a data dependent hyperparameter\n",
    "\n",
    ")\n",
    "\n",
    "print(linear_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = torch.ones([1000,1,28,28])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 4, 7, 7])\n",
      "flattened layer input size 1000 x 196\n"
     ]
    }
   ],
   "source": [
    "cnn_result = cnn_layers(test_input)\n",
    "print(cnn_result.shape)\n",
    "print('flattened layer input size', 1000 , 'x' , cnn_result.view(1000,-1).shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.cnn_layer = cnn_layers\n",
    "        self.linear_layer = linear_layers\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.cnn_layer(x)\n",
    "        x = x.view(x.shape[0],-1)\n",
    "        x = self.linear_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 10])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Net()\n",
    "net.forward(test_input).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda is available\n"
     ]
    }
   ],
   "source": [
    "optimizer =  optim.Adam(net.parameters(), lr=0.07) # learning rate \n",
    "# defining the loss function\n",
    "criterion =  nn.CrossEntropyLoss()\n",
    "# checking if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    print('cuda is available')\n",
    "    net = net.cuda()\n",
    "    criterion = criterion.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "print(images.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x16e329c5430>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "plt.imshow(np.transpose(images[0].numpy(), (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 0.904\n",
      "[1,  4000] loss: 0.743\n",
      "[1,  6000] loss: 0.858\n",
      "[1,  8000] loss: 2.334\n",
      "[1, 10000] loss: 2.329\n",
      "[1, 12000] loss: 2.329\n",
      "[1, 14000] loss: 2.329\n",
      "[2,  2000] loss: 2.330\n",
      "[2,  4000] loss: 2.330\n",
      "[2,  6000] loss: 2.332\n",
      "[2,  8000] loss: 2.329\n",
      "[2, 10000] loss: 2.328\n",
      "[2, 12000] loss: 2.329\n",
      "[2, 14000] loss: 2.326\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './fmnist_test.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loadNet = Net()\n",
    "loadNet.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (cnn_layer): Sequential(\n",
      "    (0): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (linear_layer): Sequential(\n",
      "    (0): Linear(in_features=196, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(loadNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.children at 0x0000016E0A4BA270>\n",
      "torch.Size([4, 1, 3, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x16e09f57100>"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_children = loadNet.children()\n",
    "print(model_children)\n",
    "filters = list(model_children)[0][0].weight\n",
    "print(filters.shape)\n",
    "plt.imshow(np.transpose(filters[0].cpu().detach().numpy(), (1, 2, 0)), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
