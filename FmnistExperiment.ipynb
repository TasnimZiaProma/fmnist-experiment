{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of training data  60000\n"
     ]
    }
   ],
   "source": [
    "# datasets\n",
    "trainset = torchvision.datasets.FashionMNIST('./data',\n",
    "    download=False,\n",
    "    train=True,\n",
    "    transform=transform)\n",
    "print(' Number of training data ', len(trainset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of testing data  10000\n"
     ]
    }
   ],
   "source": [
    "testset = torchvision.datasets.FashionMNIST('./data',\n",
    "    download=False,\n",
    "    train=False,\n",
    "    transform=transform)\n",
    "print(' Number of testing data ',len(testset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU(inplace=True)\n",
      "  (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (4): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (5): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (6): ReLU(inplace=True)\n",
      "  (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "cnn_layers =  nn.Sequential(\n",
    "            # Defining a 2D convolution layer\n",
    "            # torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "            # in_channels (int) – Number of channels in the input image. For B&W it is 1.\n",
    "            # out_channels (int) – Number of channels produced by the convolution. 4 filters\n",
    "            # kernel_size (int or tuple) – Size of the convolving kernel (3x3)\n",
    "            # stride (int or tuple, optional) – Stride of the convolution\n",
    "            # padding (int or tuple, optional) – Padding of 1 added to both sides of the input\n",
    "            # example x1 = (n, c=1 , h=28 , w=28 )\n",
    "            nn.Conv2d(1, 4, kernel_size=3, stride=1, padding=1), #in_channels = 1 is a data dependent hyperparameter. It is 1 because the images are in grayscale\n",
    "            # x2 = (n, c=4 , h=28 , w=28 )\n",
    "            nn.BatchNorm2d(4), # Normalize output from the activation function. \n",
    "            nn.ReLU(inplace=True), # negative elements to zero\n",
    "            # x2 = (n, c=4 , h=28 , w=28 )\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #Stride is the number of pixels shifts over the input matrix. When the stride is 1 then we move the filters to 1 pixel at a time. When the stride is 2 then we move the filters to 2 pixels at a time and so on\n",
    "            # x3 = (n, c=4 , h=14 , w=14 )\n",
    "            # Defining another 2D convolution layer\n",
    "            nn.Conv2d(4, 4, kernel_size=3, stride=1, padding=1),\n",
    "            # x3 = (n, c=4 , h=14 , w=14 )\n",
    "            nn.BatchNorm2d(4), # 4 features\n",
    "            nn.ReLU(inplace=True), # inplace = True will modify the input directly, without allocating any additional output.\n",
    "            # x3 = (n, c=4 , h=14 , w=14 )\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), # Downsamples the input representation by taking the maximum value\n",
    "            # x4 = (n, c=4 , h=7 , w=7 )\n",
    "        )\n",
    "        # outputSize = floor[(inputSize - filterSize + 2 * padding) / stride] + 1\n",
    "print(cnn_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=196, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "linear_layers = nn.Sequential(\n",
    "            nn.Linear(4 * 7 * 7, 10) \n",
    "#10 outputs because MNIST Fashion has 10 different classes. It is a data dependent hyperparameter\n",
    "\n",
    ")\n",
    "\n",
    "print(linear_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = torch.ones([1000,1,28,28])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 4, 7, 7])\n",
      "flattened layer input size 1000 x 196\n"
     ]
    }
   ],
   "source": [
    "cnn_result = cnn_layers(test_input)\n",
    "print(cnn_result.shape)\n",
    "print('flattened layer input size', 1000 , 'x' , cnn_result.view(1000,-1).shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.cnn_layer = cnn_layers\n",
    "        self.linear_layer = linear_layers\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.cnn_layer(x)\n",
    "        x = x.view(x.shape[0],-1)\n",
    "        x = self.linear_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 10])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Net()\n",
    "net.forward(test_input).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda is available\n"
     ]
    }
   ],
   "source": [
    "optimizer =  optim.Adam(net.parameters(), lr=0.07) # learning rate \n",
    "# defining the loss function\n",
    "criterion =  nn.CrossEntropyLoss()\n",
    "# checking if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    print('cuda is available')\n",
    "    net = net.cuda()\n",
    "    criterion = criterion.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "print(images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x19903fc5670>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "plt.imshow(np.transpose(images[0].numpy(), (1, 2, 0)), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 2.324\n",
      "[1,  4000] loss: 2.328\n",
      "[1,  6000] loss: 2.330\n",
      "[1,  8000] loss: 2.328\n",
      "[1, 10000] loss: 2.328\n",
      "[1, 12000] loss: 2.329\n",
      "[1, 14000] loss: 2.332\n",
      "[2,  2000] loss: 2.328\n",
      "[2,  4000] loss: 2.328\n",
      "[2,  6000] loss: 2.329\n",
      "[2,  8000] loss: 2.328\n",
      "[2, 10000] loss: 2.329\n",
      "[2, 12000] loss: 2.329\n",
      "[2, 14000] loss: 2.330\n",
      "[3,  2000] loss: 2.325\n",
      "[3,  4000] loss: 2.326\n",
      "[3,  6000] loss: 2.327\n",
      "[3,  8000] loss: 2.327\n",
      "[3, 10000] loss: 2.328\n",
      "[3, 12000] loss: 2.327\n",
      "[3, 14000] loss: 2.329\n",
      "[4,  2000] loss: 2.328\n",
      "[4,  4000] loss: 2.328\n",
      "[4,  6000] loss: 2.330\n",
      "[4,  8000] loss: 2.329\n",
      "[4, 10000] loss: 2.329\n",
      "[4, 12000] loss: 2.330\n",
      "[4, 14000] loss: 2.328\n",
      "[5,  2000] loss: 2.328\n",
      "[5,  4000] loss: 2.331\n",
      "[5,  6000] loss: 2.329\n",
      "[5,  8000] loss: 2.327\n",
      "[5, 10000] loss: 2.328\n",
      "[5, 12000] loss: 2.328\n",
      "[5, 14000] loss: 2.331\n",
      "[6,  2000] loss: 2.327\n",
      "[6,  4000] loss: 2.329\n",
      "[6,  6000] loss: 2.329\n",
      "[6,  8000] loss: 2.329\n",
      "[6, 10000] loss: 2.329\n",
      "[6, 12000] loss: 2.332\n",
      "[6, 14000] loss: 2.329\n",
      "[7,  2000] loss: 2.327\n",
      "[7,  4000] loss: 2.330\n",
      "[7,  6000] loss: 2.327\n",
      "[7,  8000] loss: 2.330\n",
      "[7, 10000] loss: 2.328\n",
      "[7, 12000] loss: 2.329\n",
      "[7, 14000] loss: 2.329\n",
      "[8,  2000] loss: 2.328\n",
      "[8,  4000] loss: 2.327\n",
      "[8,  6000] loss: 2.326\n",
      "[8,  8000] loss: 2.326\n",
      "[8, 10000] loss: 2.329\n",
      "[8, 12000] loss: 2.331\n",
      "[8, 14000] loss: 2.328\n",
      "[9,  2000] loss: 2.326\n",
      "[9,  4000] loss: 2.327\n",
      "[9,  6000] loss: 2.329\n",
      "[9,  8000] loss: 2.329\n",
      "[9, 10000] loss: 2.329\n",
      "[9, 12000] loss: 2.329\n",
      "[9, 14000] loss: 2.326\n",
      "[10,  2000] loss: 2.330\n",
      "[10,  4000] loss: 2.328\n",
      "[10,  6000] loss: 2.326\n",
      "[10,  8000] loss: 2.328\n",
      "[10, 10000] loss: 2.330\n",
      "[10, 12000] loss: 2.326\n",
      "[10, 14000] loss: 2.327\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.4464,  0.1405, -0.3777, -0.0677,  0.0247,  0.1026, -0.5148,  0.2474,\n",
      "         0.2994,  0.1011], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor(0.4464, device='cuda:0', grad_fn=<MaxBackward1>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([[0.4464, 0.2994],\n",
       "        [0.4464, 0.2994],\n",
       "        [0.4464, 0.2994],\n",
       "        [0.4464, 0.2994]], device='cuda:0', grad_fn=<TopkBackward>),\n",
       "indices=tensor([[0, 8],\n",
       "        [0, 8],\n",
       "        [0, 8],\n",
       "        [0, 8]], device='cuda:0'))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(outputs[1])\n",
    "print(torch.max(outputs[0]))\n",
    "torch.topk(outputs,k=2, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './fmnist_test.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loadNet = Net()\n",
    "loadNet.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (cnn_layer): Sequential(\n",
      "    (0): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (linear_layer): Sequential(\n",
      "    (0): Linear(in_features=196, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(loadNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.children at 0x00000199137A4430>\n",
      "torch.Size([4, 1, 3, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x19921493ac0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_children = loadNet.children()\n",
    "print(model_children)\n",
    "filters = list(model_children)[0][0].weight\n",
    "print(filters.shape)\n",
    "plt.imshow(np.transpose(filters[0].cpu().detach().numpy(), (1, 2, 0)), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
